{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L3 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know something about our data (e.g. \"it is an image,\" or, \"it is a sequence of things\") we can do better than just a general neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If color doesn't matter, it is easier to train on the greyscale. Taking the average (R+G+B)/3 is one way to do this; another would be to convert to YUV (?) and only use the Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we can explicitly build in translation invariance?\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.36.03_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the meaning of \"kitten\" change depending on which sentence it is in? It might depend on context within the sentence, but we don't want to build \"sentence 1\" and \"sentence 2\" classifiers that mist independently learn what \"kitten\" means.\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.37.11_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve this in neural networks with _weight sharing_:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.38.17_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Statistical invariants\" - things that don't change across time or space - are everywhere and we want to train weights jointly across samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For images, this will lead us to convolutional neural networks. For text, it will lead us to embeddings and recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Convnets\" are networks that share their parameters across space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-22_at_8.42.15_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a tiny neural network on a patch of the image and represent the outputs as a vector of length `k`:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.43.18_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's slide that network across the image without changing the weights. In the output, this creates another image:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.44.52_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new image has a different width, height and depth (`k`). This operation is called a _convolution_. Because we use a small patch, we have many fewer weights than if we had used the entire image for our \"patch\". What's more, these weights are shared across space. Instead of stacks of generic layers, we're going to use stacks of these convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we build a \"convolutional pyramid\", we eventually squeeze out all the spatial information and eventually only have classification:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.48.36_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing depth -> Increasing semantic representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import convnet \"lingo\": here we are mapping 3 feature maps to `k` feature maps:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.51.53_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Stride\" tells us about how many pixels we are shifting our patch/kernel at each step of the sweep:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.53.04_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"valid padding\" (don't go off the edge)\n",
    "* \"same padding\" (can go off the edge)\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.54.21_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Map Sizes Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-22_at_8.56.04_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Padding    Stride     Width    Height    Depth\n",
    "    'Same'     1          28       28        8\n",
    "    'Valid'    1          26       26        8\n",
    "    'Valid'    2          13       13        8\n",
    "    \n",
    "    Padding    Stride     Width       Height      Depth\n",
    "    'Same'     1          28          28          8\n",
    "    'Valid'    1          28-3+1      28-3+1      8\n",
    "    'Valid'    2          (28-3+1)/2  (28-3+1)/2  8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src='Screen_Shot_2016-02-23_at_8.14.37_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to training when we use shared weights? We just add up the gradients for every patch.\n",
    "\n",
    "<img src='Screen_Shot_2016-02-23_at_8.17.08_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Design Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced convnet-ology\n",
    "\n",
    "* pooling\n",
    "* $1 \\times 1$ convolutions (one by one)\n",
    "* inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using higher stride to reduce dimensionality is very \"aggressive\" - we can lose a lot of information. It is better to keep all the convolutions, but combine them somehow - this operation is called \"pooling\".\n",
    "\n",
    "<img src='Screen_Shot_2016-02-23_at_8.18.58_AM.png'>\n",
    "\n",
    "Max pooling is more accurate, but computationally expensive (lower-layer convolutions are still running at a lower stride). Note that pooling size and stride do not need to be the same.\n",
    "\n",
    "<img src='Screen_Shot_2016-02-23_at_8.21.01_AM.png'>\n",
    "\n",
    "<img src='Screen_Shot_2016-02-23_at_8.23.11_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average pooling is another very notable form of pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 x 1 Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-23_at_8.26.04_AM.png'>\n",
    "\n",
    "The classic convolution setting - it is a _linear_ classifier over a patch of the image:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-23_at_8.26.45_AM.png'>\n",
    "\n",
    "If we add a 1x1 convolution \"in the middle\", we create a mini-neural-network to run over the patch. This network is a computationally cheap way to add parameters to the model because 1x1 convoltions are really just matrix multiplications.\n",
    "\n",
    "<img src='Screen_Shot_2016-02-23_at_8.29.34_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each layer of the network, we make a choice - do we want pooling, maybe 5x5, maybe 3x3, maybe 1x1 convolutions. All are potentially beneficial to the network. But why choose? - Let's use them all!\n",
    "\n",
    "<img src='Screen_Shot_2016-02-23_at_8.32.48_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lot's of building blocks - easy to make interesting architectures..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Assignment: ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many architectures to try... including inception. ConvNets are where you really start to need GPUs though..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segue to Assignment 4: Convolutional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
