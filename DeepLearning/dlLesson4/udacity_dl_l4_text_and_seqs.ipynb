{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity: Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L4: Text and Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a text embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words are really hard - there are lots of them, and most of them we never see. But usually it is the rarest words that provide the most information. This is a big problem in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to share parameters between things that are similar, e.g. \"cat\" and \"kitty.\" But these words are not always similar!\n",
    "\n",
    "Big problems:\n",
    "\n",
    "1. we'd like to see the really rare words often enough that we can learn how to use them\n",
    "\n",
    "2. we need a way to relate words that are similar in semantic meaning\n",
    "\n",
    "We need lots of labeled data to do this - more labeled data than we can handle! What to do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning means training without labels. There is lots and lots of text out there (e.g., the web) _if_ we can figure out what to learn from it. There is a powerful concept we may exploit - similar words appear in similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"the **cat** purrs\"\n",
    "* \"this **cat** hunts mice\"\n",
    "\n",
    "Also reasonable to say\n",
    "\n",
    "* \"the **kitty** purrs\"\n",
    "* \"this **kitty** hunts mice\"\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.10.15_PM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope we can produce a model that predicts a word's context. A model that predicts context will have to treat \"cat\" and \"kitty\" similarly and tend to bring them closer together.\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.12.13_PM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helps us solve the sparsity and representation problem once we have encoded our words into vectors called _embeddings_. Now all cat-like things will be similar, etc. The model no longer has to learn new things for every way there is to use a word - we get _generalization_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.16.21_PM.png'>\n",
    "\n",
    "1. We map each word in the sentance into an embedding - initially a random one.\n",
    "2. We will use the embedding to try to predict the context of the word.\n",
    "3. In this model, the context is the words that are nearby (within a window). We choose a random word in a window around our original word, and that becomes our target.\n",
    "4. Then, we train our model as though it were a supervised problem. We use _logistic classifiers_ to make our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be good to see that our training was working by clustering similar words closer together. One way to do this would be to do a nearest neighbors look-up. Another way would be to reduce the dimensionality of the representation and project into two dimensions. But we need a way of doing this that preserves the neighborhood structure (things that are close in embedding space should still be close in 2D, and things that are far should remain far). This means PCA ends up not working so well, but a technique called \"t-SNE\" works well.\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.24.07_PM.png'>\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.25.40_PM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-03-01_at_6.26.33_PM.png'>\n",
    "\n",
    "The cosine distance is generally a better measure than $L_2$ - this because the length of the embedding vector is not relevant to the classification. In fact, it is often better to normalize all embedding vectors to have unit norm.\n",
    "\n",
    "We can also use _sampled softmax_ to make our computations easier:\n",
    "\n",
    "<img src='Screen_Shot_2016-03-01_at_6.29.29_PM.png'>\n",
    "\n",
    "The idea behind sampled softmax is rather than treat our target vector such that the target word has probability 1 and every other word has probability 0, we only sample a small random subset of the whole rest of the vocabulary, and pretend the other words aren't there. This makes computations much more efficient and is almost as good as using the full vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word Analogy Game Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-03-04_at_7.52.57_AM.png'>\n",
    "\n",
    "* kitten\n",
    "* shorter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saying a puppy is to a dog as what a kitten is to a cat is an example of a _semnatic analogy_. Saying taller is to tall as shorter is to short is an example of a _syntactic analogy_.\n",
    "\n",
    "<img src='Screen_Shot_2016-03-04_at_7.59.43_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segue to Assignment 5: Word2Vec and CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Word2Vec and use it as a refernce to train a continuous bag of words (CBOW) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
