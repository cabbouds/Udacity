{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L3 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro Lesson 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know something about our data (e.g. \"it is an image,\" or, \"it is a sequence of things\") we can do better than just a general neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If color doesn't matter, it is easier to train on the greyscale. Taking the average (R+G+B)/3 is one way to do this; another would be to convert to YUV (?) and only use the Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we can explicitly build in translation invariance?\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.36.03_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the meaning of \"kitten\" change depending on which sentence it is in? It might depend on context within the sentence, but we don't want to build \"sentence 1\" and \"sentence 2\" classifiers that mist independently learn what \"kitten\" means.\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.37.11_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve this in neural networks with _weight sharing_:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.38.17_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Statistical invariants\" - things that don't change across time or space - are everywhere and we want to train weights jointly across samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For images, this will lead us to convolutional neural networks. For text, it will lead us to embeddings and recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Convnets\" are networks that share their parameters across space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-22_at_8.42.15_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a tiny neural network on a patch of the image and represent the outputs as a vector of length `k`:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.43.18_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's slide that network across the image without changing the weights. In the output, this creates another image:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.44.52_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new image has a different width, height and depth (`k`). This operation is called a _convolution_. Because we use a small patch, we have many fewer weights than if we had used the entire image for our \"patch\". What's more, these weights are shared across space. Instead of stacks of generic layers, we're going to use stacks of these convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we build a \"convolutional pyramid\", we eventually squeeze out all the spatial information and eventually only have classification:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.48.36_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing depth -> Increasing semantic representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import convnet \"lingo\": here we are mapping 3 feature maps to `k` feature maps:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.51.53_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Stride\" tells us about how many pixels we are shifting our patch/kernel at each step of the sweep:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.53.04_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"valid padding\" (don't go off the edge)\n",
    "* \"same padding\" (can go off the edge)\n",
    "\n",
    "<img src='Screen_Shot_2016-02-22_at_8.54.21_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Map Sizes Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-22_at_8.56.04_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Padding    Stride     Width    Height    Depth\n",
    "    'Same'     1          28       28        8\n",
    "    'Valid'    1          26       26        8\n",
    "    'Valid'    2          13       13        8\n",
    "    \n",
    "    Padding    Stride     Width       Height      Depth\n",
    "    'Same'     1          28          28          8\n",
    "    'Valid'    1          28-3+1      28-3+1      8\n",
    "    'Valid'    2          (28-3+1)/2  (28-3+1)/2  8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
