{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2: Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Parameters Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-15_at_8.27.37_AM.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7850\n"
     ]
    }
   ],
   "source": [
    "W_n_in = 28 * 28 \n",
    "W_n_out = 10\n",
    "n_W = W_n_in * W_n_out\n",
    "n_bias = 10\n",
    "n_total = n_bias + n_W\n",
    "print(n_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Models are Limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-15_at_8.36.02_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, linear models, are, well, _linear_. We cannot represent certain types of outcomes. But, linear models are efficient and stable! The derivatives are really nice too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Units (ReLU) Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice derivative..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Network of ReLUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum change: add a layer of $H$ ReLUs in the \"middle\" of our network. So now we have two sets of weights and biases, with a layer of ReLUs in between:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-16_at_8.12.29_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Neuromorphic engineering\" - great phrase, and it might even be true. But it comes with a lot of baggage. So we are going to avoid that sort of language here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our network by stacking simple operations. This allows us to employ the chain rule:\n",
    "\n",
    "\\begin{equation}\n",
    "[g(f(x))]' = g'(f(x)) \\times f'(x)\n",
    "\\end{equation}\n",
    "\n",
    "We may represent this in a _graph_:\n",
    "\n",
    "<img src='Screen_Shot_2016-02-16_at_8.18.58_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-16_at_8.21.10_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure:\n",
    "\n",
    "1. run the forward prop\n",
    "2. run the back prop\n",
    "3. update weights according to $W_i \\to W_i - \\alpha \\Delta W_i$\n",
    "4. go back to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each block of the back prop usually takes about twice the memory as the corresponding block in the forward prop, and twice the compute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segue into Assignment 2: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training a Deep Learning Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep networks have several advantages over \"wide\" networks:\n",
    "\n",
    "1. parameter efficiency - better performance with fewer parameters by going deeper rather than wider\n",
    "2. deep models naturally capture hierarchical structure (lower levels find line edges, higher levels find parts, final levels identify objects)\n",
    "\n",
    "<img src='Screen_Shot_2016-02-17_at_8.36.12_AM.png'>\n",
    "<img src='Screen_Shot_2016-02-17_at_8.38.21_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Deep models require a lot of data. Another reason they work well today is regularization. Optimizing network size is hard, so we typically train networks that are too large and then try to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-18_at_8.01.14_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is applying artifical constraints on the network that artificially reduce the number of free paramters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"stretch pants of deep learning are called L2 regularization\". The idea is to add another term to the loss that penalizes large weights. This does mean we have yet another hyperparameter to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-18_at_8.04.44_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Screen_Shot_2016-02-18_at_8.06.14_AM.png'>\n",
    "\n",
    "The derivative is $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout (randomly zeroing out activations for each example) forces the network to learn _redundant_ representations. It also makes the network act as if it is taking the consensus of an ensemble of networks.\n",
    "\n",
    "<img src='Screen_Shot_2016-02-18_at_8.09.00_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout pt. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating a network trained with dropout, we no longer want the randomness. We want the consensus. So we want to average over the activations. This means that during training, we not only remove activations, but we also scale up the responses of the other activations (to keep properties of the mean valid in evaluation).\n",
    "\n",
    "<img src='Screen_Shot_2016-02-18_at_8.13.29_AM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segue into assignment on regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
